# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D57VJIiZwRYtT31JDHB1wxeCuTlUKQo3
"""

import pandas as pd
import requests
import base64
from PIL import Image
from io import BytesIO
from PIL import ImageEnhance
import matplotlib.pyplot as plt
import numpy as np

file_path = '/content/A2_Data - A2_Data.csv'
column_names = ['Product ID.', 'Image', 'Review text']
df = pd.read_csv(file_path,names=column_names)
df = df.drop(0)
df = df.reset_index(drop=True)

df

image_column = "Image"
for index, row in df.iterrows():
    image_url = row[image_column]
    images = image_url.strip('[]').split(',')
    images = [value.strip().strip("'") for value in images]
    df.at[index, image_column] = images

df = df.explode('Image')
df.reset_index(drop=True, inplace=True)
df["image_links"] = df["Image"]

def url_to_image(url):

    try:
        response = requests.get(url)
        if response.status_code == 200:
            img = Image.open(BytesIO(response.content))
            return img
        else:
            return None
    except Exception as e:
        print(f"Error downloading image from {url}: {e}")
        return None
def preprocess_images(url):
    preprocessed_images = []

    image = url_to_image(url)
    if image is not None:
        preprocessed_images.append(image)
    return preprocessed_images

# Apply preprocessing and filter rows with no valid images
df['Image'] = df['Image'].apply(preprocess_images)
df = df[df['Image'].apply(len) > 0]

def adjust_contrast(image, factor=1.2):
    enhancer = ImageEnhance.Contrast(image)
    return enhancer.enhance(factor)

# Function to resize an image to a specified size
def resize_image(image, size=(100, 100)):
    return image.resize(size)

# Function to adjust brightness and exposure of an image
def adjust_brightness(image, factor=1.2):
    enhancer = ImageEnhance.Brightness(image)
    return enhancer.enhance(factor)

def preprocessing(urls):
  preprocessed_images = []
  for image in urls:
        image = adjust_contrast(image)
        image = resize_image(image)
        image = adjust_brightness(image)
        preprocessed_images.append(image)
  return preprocessed_images

df['Image'] = df['Image'].apply(preprocessing)

from keras.applications import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import pickle

model = VGG16(weights='imagenet', include_top=False)
def normalize_features(features):
    min_val = np.min(features)
    max_val = np.max(features)
    normalized_features = (features - min_val) / (max_val - min_val)
    return normalized_features

def extract_features(images):
    image_features = []
    for img in images:
            img = img.resize((100, 100))  # Resize image to VGG16 input size
            x = image.img_to_array(img)
            x = np.expand_dims(x, axis=0)
            x = preprocess_input(x)
            features = model.predict(x)
            image_features.append(features)  # Store extracted features
    return image_features
df['Image_Features'] = df['Image'].apply(extract_features)


df['Image_Features'] = df['Image_Features'].apply(lambda x: [normalize_features(feature) for feature in x])


import pickle
with open('/content/extracted_features.pkl', 'wb') as f:
    pickle.dump(df['Image_Features'], f)

with open('/content/extracted_features.pkl', 'rb') as f:
    data = pickle.load(f)
    df['Image_Features'] = data

"""#Part 2"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import string
from nltk.tokenize import TweetTokenizer
import re

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
tokenizer = TweetTokenizer()
patterns = r"\b(?:[a-zA-Z](?:[a-zA-Z'_]*[a-zA-Z])?|(?<![_-])['_-](?![_-]))\b"

def preprocess_text(text):
    # Lower-casing
    if pd.isnull(text) or text.strip() == '':
        return None
    lowercase_content = text.lower()
    tokens = re.findall(patterns, lowercase_content)
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [token for token in tokens if token not in string.punctuation]
    tokens = [token for token in tokens if token.strip()]


    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(token) for token in tokens]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return tokens

# Apply text preprocessing to the 'Review text' column
df['Processed Review'] = df['Review text'].apply(preprocess_text)

df.info()

from collections import Counter
def calculate_tf(review):
    tf_dict = {}
    if review == None:
        return None

    word_count = len(review)
    word_freq = Counter(review)
    for word, freq in word_freq.items():
        tf_dict[word] = freq / word_count
    return tf_dict

df['TF'] = df['Processed Review'].apply(calculate_tf)

import math
def calculate_idf(corpus):
    total_docs = len(corpus)
    idf_dict = {}
    word_doc_freq = {}
    for doc in corpus:
        if doc is None:
            continue
        unique_words = set(doc)
        for word in unique_words:
            word_doc_freq[word] = word_doc_freq.get(word, 0) + 1
    for word, freq in word_doc_freq.items():
        idf_dict[word] = math.log(total_docs / freq)
    return idf_dict

corpus = df['Processed Review'].tolist()
idf_dict = calculate_idf(corpus)

idf_dict

def calculate_tfidf(tf_dict, idf_dict):
    if not tf_dict:  # Check if tf_dict is empty (None review)
        return {}
    tfidf_dict = {}
    for word, tf in tf_dict.items():
        tfidf_dict[word] = tf * idf_dict.get(word, 0)
    return tfidf_dict

df['TF-IDF'] = df['TF'].apply(lambda x: calculate_tfidf(x, idf_dict))

with open('/content/tf-idf.pkl', 'wb') as f:
    pickle.dump(df['TF-IDF'], f)

"""#Part 3

"""

def download_image(url):
    response = requests.get(url)
    if response.status_code == 200:
        img = Image.open(BytesIO(response.content))
        return img
    else:
        print(f"Failed to download image from URL: {url}")
        return None

input_image_url = input("enter url of the image: ")
input_image = download_image(input_image_url)

input_image

img = input_image.resize((100, 100))  # Resize image to VGG16 input size
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
features = model.predict(x)
features = normalize_features(features)

def flatten_array(lst):
    arr = np.array(lst)
    arr = arr.reshape(-1)
    return arr

# Apply the function to each element of the DataFrame column
df['Image_Features'] = df['Image_Features'].apply(flatten_array)
features = features.reshape(-1)

row = df[df['image_links'] == input_image_url]

# Retrieve the product ID from the row
if not row.empty:
    product_id = row.iloc[0]['Product ID.']
    product_review = row.iloc[0]['Processed Review']
    tfidf = row.iloc[0]['TF-IDF']
    print("Product ID:", product_id)
else:
    print("Image link not found in DataFrame.")

def dict_to_vector(dictionary, vocabulary):
    vector_length = len(vocabulary)
    vector = np.zeros(vector_length)
    for key, value in dictionary.items():
        if key in vocabulary:
            index = vocabulary.index(key)
            vector[index] = value
    return vector

product_id

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming 'features' and 'product_id' are already defined
flat_input_features = features.flatten()

# Calculate cosine similarity between input features and each array in the DataFrame column
cos_similarities = []

# Store product IDs of images already considered

for index, row in df.iterrows():
    # Exclude images with the same product ID as the input image
    if row['Product ID.'] == product_id:
        continue
    if row['Product ID.'] == product_id:
        print(product_id)
        continue

    # Check if the product ID has already been considered


    resized_array = np.resize(row['Image_Features'], features.shape)
    flat_array = resized_array.flatten()
    cos_sim = cosine_similarity(flat_input_features.reshape(1, -1), flat_array.reshape(1, -1))
    cos_similarities.append((index, cos_sim[0][0]))

    # Add the product ID to the set of considered IDs

# Sort similarity scores in descending order
cos_similarities.sort(key=lambda x: x[1], reverse=True)

# Retrieve top three most similar images
top_three_similar_images = cos_similarities[:3]
top_three_similar_images



review_cosine = []
for index, similarity_score in top_three_similar_images:
  tfidf_dict1 = tfidf
  tfidf_dict2 = df.loc[index,'TF-IDF']
  vocabulary = list(set(tfidf_dict1.keys()) | set(tfidf_dict2.keys()))
  tfidf_vector1 = dict_to_vector(tfidf_dict1, vocabulary)
  tfidf_vector2 = dict_to_vector(tfidf_dict2, vocabulary)
  cosine_sim = cosine_similarity([tfidf_vector1], [tfidf_vector2])
  review_cosine.append(cosine_sim[0][0])




# Print top three similar images along with their URLs, product reviews, and product IDs
i=0
for index, similarity_score in top_three_similar_images:
    image_link = df.loc[index, 'image_links']
    product_review = df.loc[index, 'Review text']
    product_id = df.loc[index, 'Product ID.']
    print("Product ID:", product_id)
    print("Image link:", image_link)
    print("Product review:", product_review)
    print("Cosine similarity score of image:", similarity_score)
    print("Cosine similarity score of text:",review_cosine[i])
    i+=1
    print()

"""Ranking on the basis of the composite score"""

composite_scores = [(index, (similarity_score + review_cosine[i]) / 2) for i, (index, similarity_score) in enumerate(top_three_similar_images)]
composite_scores = sorted(composite_scores, key=lambda x: x[1], reverse=True)

for index, composite_score in composite_scores:
    image_link = df.loc[index, 'image_links']
    product_review = df.loc[index, 'Review text']
    product_id = df.loc[index, 'Product ID.']
    print("Product ID:", product_id)
    print("Image link:", image_link)
    print("Product review:", product_review)
    print("Composite similarity score:", composite_score)
    print()

input_image_review= input("enter review of the image: ")
input_image_review = preprocess_text(input_image_review)
row = df[df['Processed Review'].apply(lambda x: x == input_image_review)]


# Retrieve the product ID from the row
if not row.empty:
    product_id = row.iloc[0]['Product ID.']
    product_url = row.iloc[0]['image_links']
    tfidf = row.iloc[0]['TF-IDF']
    input_features = row.iloc[0]['Image_Features']
    print("Product ID:", product_id)
    print("product url:",product_url)
else:
    print("Image review not found in DataFrame.")

product_id

from sklearn.metrics.pairwise import cosine_similarity
cos_similarities = []

# Store product IDs of images already considered
considered_ids = set()
considered_ids.add(product_id)
print(considered_ids)

for index, row in df.iterrows():
    # Exclude images with the same product ID as the input image
    if row['Product ID.'] in considered_ids:
      continue
    if row['Product ID.'] in considered_ids:
      continue
    considered_ids.add(row['Product ID.'])
    if row['Product ID.'] == product_id:
      print("hi")

    tfidf_dict1 = tfidf
    tfidf_dict2 = row['TF-IDF']
    vocabulary = list(set(tfidf_dict1.keys()) | set(tfidf_dict2.keys()))


    tfidf_vector1 = dict_to_vector(tfidf_dict1, vocabulary)
    tfidf_vector2 = dict_to_vector(tfidf_dict2, vocabulary)



    cosine_sim = cosine_similarity([tfidf_vector1], [tfidf_vector2])
    cos_similarities.append((index, cosine_sim[0][0]))


# Sort similarity scores in descending order
cos_similarities.sort(key=lambda x: x[1], reverse=True)


top_three_similar_reviews = cos_similarities[:3]
top_three_similar_reviews

input_features = np.array(input_features)
input_features=input_features.flatten()
image_cos_sim = []
for index,similarity_score in top_three_similar_reviews:
    resized_array = np.resize(df.loc[index,'Image_Features'], input_features.shape)
    flat_array = resized_array.flatten()
    cos_sim = cosine_similarity(input_features.reshape(1, -1), flat_array.reshape(1, -1))
    image_cos_sim.append(cos_sim[0][0])


# Print top three similar images along with their URLs, product reviews, and product IDs
i=0
for index, similarity_score in top_three_similar_reviews:
    image_link = df.loc[index, 'image_links']
    product_review = df.loc[index, 'Review text']
    product_id = df.loc[index, 'Product ID.']
    image = df.loc[index,'image_links']
    print("Product ID:", product_id)
    print("Product review:", product_review)
    print("Product Image:", image)
    print("Cosine similarity score of text:",similarity_score)
    print("Cosine similarity score of Image:",image_cos_sim[i])
    i+=1
    print()

composite_scores = [(index, (similarity_score + review_cosine[i]) / 2) for i, (index, similarity_score) in enumerate(top_three_similar_reviews)]
composite_scores = sorted(composite_scores, key=lambda x: x[1], reverse=True)

for index, composite_score in composite_scores:
    image_link = df.loc[index, 'image_links']
    product_review = df.loc[index, 'Review text']
    product_id = df.loc[index, 'Product ID.']
    print("Product ID:", product_id)
    print("Image link:", image_link)
    print("Product review:", product_review)
    print("Composite similarity score:", composite_score)
    print()




